[["index.html", "Introduction to econometrics Chapter 1 Introduction", " Introduction to econometrics Boyko Amarov 2021-01-11 Chapter 1 Introduction A summary of class notes on time series analysis 2020/2021. library(xts) ## Loading required package: zoo ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric "],["covariance.html", "Chapter 2 Sample covariance and sample correlation 2.1 Time series analysis class 2 2.2 Purely random process (white noise) 2.3 Simulating white noise 2.4 Sample (empirical) covariance 2.5 Exercise", " Chapter 2 Sample covariance and sample correlation 2.1 Time series analysis class 2 Let us denote the realisations of a time series process with \\[ y: y_1, y_2,\\ldots,y_T \\] where \\(y_1\\) is the first value of the series and \\(y_T\\) is the last value of the series. The first lag of the series is defined as \\[ y_{t - 1}: \\text{ first lag} \\] 2.2 Purely random process (white noise) Let \\(u_t, t = 1\\ldots,T\\) be an uncorrelated, normally distributed zero mean random variables with constant variance \\(\\sigma^2\\). \\[ u_{t} \\sim N(0, \\sigma^2), \\quad t=1,\\ldots,T \\] We call \\(u_{t}\\) a purely random or white noise process. \\[ E(u_{t}) = 0\\\\ Var(u_{t}) = \\sigma^2\\\\ Cov(u_{t}, u_{t - k}) = 0 \\quad \\text{for} \\quad k \\neq 0 \\] 2.3 Simulating white noise Generate 100 values from a standard normal distribution (i.e. \\(\\sigma^2 = 1\\)) and create an arbitrary time index (e.i. 100 days starting from 2018-10-10). library(xts) ## Select 100 values at random from the standard normal distribution (the defaults in rnorm are mean = 0 and sd = 1). randomValues &lt;- rnorm(100) ## Create a time index for xts timeIndex &lt;- seq.Date(as.Date(&quot;2018-10-10&quot;), by = &quot;day&quot;, length.out = 100) ## Combine the values and the time index u &lt;- xts(randomValues, timeIndex) ## Plot the series plot(u) From the plot you can wee that the series fluctuates around its mean (close to 0) and appears to have a constant variance. Most of the values lie between -2 and 2 as expected for values selected from a standard normal distribution. \\[ X \\sim N(\\mu, \\sigma^2)\\\\ \\text{ roughly } 95\\% \\text{ of the realisations of X are expected to be in the interval} \\\\ [\\mu - 2\\sigma, \\mu + 2\\sigma] \\\\ \\text{for } \\mu = 1, \\sigma^2 = 1\\quad (\\text{standard normal distribution})\\\\ \\text{we obtain } [0 - 2*1, 0 + 2 * 1] = [-2, 2] \\] To examine the correlation between the series and its first lag we create a scatter-plot of \\(u_{t}\\) and \\(u_{t - 1}\\). ## Create the first lag (u_{t - 1}) uL1 &lt;- lag(u) ## Next we combine the original series and the lag so that it is easier for plotting. combinedSeries &lt;- cbind(u, uL1) ## Finally we create the scatter-plot plot(as.data.frame(uL1)[, 1], as.data.frame(u)[, 1], xlab = &quot;u_{t - 1}&quot;, ylab = &quot;u_{t}&quot;, main = &quot;Scatterplot of u_t and u_{t - 1}&quot;) The plot indicates no association between \\(u_{t}\\) and \\(u_{t - 1}\\). 2.4 Sample (empirical) covariance A key statistic for measuring the linear association between time series is the correlation coefficient. To define it we first examine the sample covariance between two series \\(x_{t}\\) and \\(y_{t}\\). Let \\(x\\) and \\(y\\) be two random variables with \\(T\\) realisations. \\[ x: x_{1}, x_{2}, \\ldots, x_{T}\\\\ y: y_{1}, y_{2}, \\ldots, y_{T}\\\\ \\] The sample covariance is defined as: \\[ Cov(x, y) = \\frac{1}{T - 1}\\sum_{t = 1}^{T}(x_t - \\overline{x})(y_{t} - \\overline{y}) \\] In order to see how it works we will examine the results from a couple of simulations. 2.4.1 Positive linear dependency First, let us define the relationship between \\(x_{t}\\) and \\(y_{t}\\) as follows. \\[ y_{t} = 1 + x_{t} + u_{t}, \\quad u_{t} \\sim WN(\\sigma^2 = 5) \\] The above equation simply tells us that the value of \\(y_{t}\\) is simply the value of \\(x_{t}\\) plus one and the result of a normally distributed random variable. For simplicity, let \\(x_{t} = t\\). ## Generate a vector of values for x: the integers from 1 to 50. x &lt;- 1:50 ## Draw values at random from a normal distribution with mean zero and variance 25 noise &lt;- 5*rnorm(50) ## Finally, generate the value for y according to the equation above y &lt;- 1 + x + noise ## Plot x and y plot(x, y) ## Compute the mean of x and the mean of y mean(x) ## [1] 25.5 mean(y) ## [1] 27.41491 ## Draw a (red) vertical line at the mean of x abline(v = mean(x), col = 3, lwd = 2) ## Draw a (green) horizontal line at the mean of y abline(h = mean(y), col = 2, lwd = 2) From the plot you will notice that most of the points fall in the lower-left and in the upper-right parts of the plot. Next examine the sign of the cross products \\((x_t - \\overline{x})(y_t - \\overline{y})\\) that we find in the definition of the covariance. \\[ \\overline{x} = \\frac{1}{T}\\sum_{t = 1}^{T}x_{t} \\text{ the sample mean of } x \\\\ \\overline{y} = \\frac{1}{T}\\sum_{t = 1}^{T}y_{t} \\text{ the sample mean of } y \\\\ (x_t - \\overline{x})(y_{t} - \\overline{y}) &gt; 0 \\text{ for points in the top right part}\\\\ (x_t - \\overline{x})(y_{t} - \\overline{y}) &gt; 0 \\text{ for points in the bottom left part}\\\\ (x_t - \\overline{x})(y_{t} - \\overline{y}) &lt; 0 \\text{ for points in the bottom right and top left parts}\\\\ (y_t - \\overline{y}) &gt; 0 \\text{ for points above the red line}\\\\ (y_t - \\overline{y}) &lt; 0 \\text{ for points below the red line}\\\\ (x_t - \\overline{x}) &gt; 0 \\text{ for points to the right of the green line}\\\\ (x_t - \\overline{x}) &lt; 0 \\text{ for points to the left of the green line} \\] You should notice that for a positive linear associaiton between \\(x\\) and \\(y\\) the cross products will be positive and therefor the covariance of \\(x\\) and \\(y\\) will be positive. You can compute the sample covariance between the \\(x\\) and \\(y\\) by using the cov function: ## Sample covariance of x and y cov(x, y) ## [1] 218.0108 To see how the sign of the covariance changes for negative linear associations between \\(x\\) and \\(y\\), change the sign before \\(x\\) and re-run the simulation. Unfortunately, the value of the covariance depends on the units of measurement of \\(x\\) and \\(y\\) (i.e. kg, mg, etc.). Try multiplying x or y or both by a constant and see how the covariance changes. cov(x, 50*y) ## [1] 10900.54 cov(10*x, y) ## [1] 2180.108 cov(10*x, 10*y) ## [1] 21801.08 The dependency of the covariance on the unit of measurement of the variables makes it difficult to distinguish between strong and weak dependence. Therefore we introduce the correlation coefficient \\(\\rho(x, y)\\). It is defined as the ratio of the covariance and the standard deviations of the variables. $$ s^2(x) = {t = 1}^{T}(x_t - )^2 \\ s^2(y) = {t = 1}^{T}(y_t - )^2 \\ s(x) = \\ s(y) = \\ (x, y) = $$ It can be shown that the correlation coefficient is always less than or equal to 1 and greater than or equal to -1. \\[ -1 \\leq \\rho(x, y) \\leq 1 \\] It equals 1 for a perfect linear association between \\(x\\) and \\(y\\). \\[ y_{t} = x_{t} \\] \\[ \\rho(x, y) = \\rho(x, x) = \\frac{Cov(x, x)}{\\sqrt{s^2(x)s^2(x)}} = \\frac{s^2(x)}{s^2(x)} = 1 \\] In the above we use the fact that the covariance between \\(x\\) and \\(x\\) is simply the variance of \\(x\\): \\[ Cov(x, x) = \\frac{1}{T - 1}\\sum_{t = 1}^{T}(x_t - \\overline{x})(x_t - \\overline{x}) =\\\\ \\frac{1}{T - 1}\\sum_{t = 1}^{T}(x_t - \\overline{x})^2 = Var(x) = s^2(x) \\] The correlation coefficient equals -1 for a perfect negative linear association: \\[ y_{t} = -x_{t} \\implies \\frac{Cov(-x, x)}{\\sqrt{s^2(-x)s^2(x)}} = -\\frac{Cov(x, x)}{\\sqrt{s^2(x)s^2(x)}} = -\\frac{s^2(x)}{s^2(x)} = -1 \\] The above result also holds for other linear associations: \\[ y = a + bx, b &gt; 0\\implies \\rho(x, y) = 1\\\\ y = a + bx, b &lt; 0\\implies \\rho(x, y) = -1\\\\ \\] ## Autocovariances and autocorrelations In the case of time series we are interested in describing the dependency pattern between the value of a series and its past values (lags). \\[ y_t, y_{t - 1}, y_{t - 2},\\ldots, y_{t - k} \\] \\[ \\gamma_0 = Cov(y_t, y_{t}) = Var(y_t) = s^2(x)\\\\ \\gamma_{1} = Cov(y_{t}, y_{t - 1}) \\text{ first order autocovariance }\\\\ \\gamma_{2} = Cov(y_{t}, y_{t - 2}) \\text{ second order autocovariance } \\\\ \\gamma_{3} =Cov(y_{t}, y_{t - 3}) \\text{ third order autocovariance } \\\\ \\vdots \\\\ \\gamma_{k} =Cov(y_{t}, y_{t - k}) \\text{ k-th order autocovariance } \\\\ \\] \\[ \\rho_{0}= \\rho(y_t, y_{t}) = 1 \\\\ \\rho_{1} = \\rho(y_{t}, y_{t - 1}) \\text{ first order autocorrelation }\\\\ \\rho_{2} = \\rho(y_{t}, y_{t - 2}) \\text{ second order autocorrelation } \\\\ \\rho_{3} =\\rho(y_{t}, y_{t - 3}) \\text{ third order autocorrelation } \\\\ \\vdots \\\\ \\rho_{k} =\\rho(y_{t}, y_{t - k}) \\text{ k-th order autocorrelation } \\] \\[ \\rho_1 = \\frac{\\gamma_1}{\\gamma_0} \\\\ \\rho_2 = \\frac{\\gamma_2}{\\gamma_0} \\\\ \\vdots\\\\ \\rho_k = \\frac{\\gamma_k}{\\gamma_0} \\] 2.5 Exercise Compute the sample first order autocovariance of the following time series. $$ x: x_1, x_2, x_3\\ x: (2, 3, 10)\\ Cov(x_t, x_{t - 1}) = {t = 1}^{T}(x_t - )(x{t - 1} - )\\ = 5\\ 1 = Cov(x_t, x{t - 1}) = [(2 - 5)(NA - 5) + (3 - 5)(2 - 5) + (10 - 5)(3 - 5)] =\\ = [NA + (-2)(-3) + (5)(-2)] = (6 -10) = -2\\ 1 = -2\\ {1} = ? \\ s^2(x) = Var(x_{t}) = _{t = 1}^{T}=\\ [(2 - 5)^2 + (3 - 5)^2 + (10 - 5)^2] = [(-3)^2 + (-2)^2 + 5^2] = \\ [9 + 4 + 25] = = 19\\ 0 = 19\\ {1} = = = … $$ We can check that our result is correct by running the same calculation with R: var(c(2, 3, 10)) ## [1] 19 acf(c(2, 3, 10), plot = FALSE) ## ## Autocorrelations of series &#39;c(2, 3, 10)&#39;, by lag ## ## 0 1 2 ## 1.000 -0.105 -0.395 "],["fit.html", "Chapter 3 Fitting arima models 3.1 Data exploration 3.2 Fit an AR(p) process 3.3 Model choice (1) 3.4 Model choice (2)", " Chapter 3 Fitting arima models Let us look at a real dataset of Bulgarian quarterly GDP figures from 2000 to 2017. Our goal will be to select a mathematical model for the quarterly GDP growth rates. library(xts) # a) Read data gdp &lt;- read.csv(&#39;https://raw.githubusercontent.com/feb-uni-sofia/timeseries-20202021/main/data/gdp_bg_qrt_2000-2017.csv&#39;) ## Create a time index from the column names &quot;Index&quot; in the gdp dataset. timeIndex &lt;- as.yearqtr(gdp$Index, format = &#39;%YQ%q&#39;) ## Combine the values of the GDP (column called &quot;GDP&quot;) and the time index to create ## a time series object. gdpSeries &lt;- xts(gdp$GDP, order.by = timeIndex) Before we compute the quarterly growth rates we will first examine the original series. plot(gdpSeries) acf(gdpSeries) A prominent feature in the dataset is a strong seasonal effect. Furthermore, the data appears to have a positive trend at least until 2009. Let us denote the data in this time series with \\[ y_{1}, y_{2}, y_{3},\\ldots,y_{T}\\\\ \\] where \\(t = 1\\) corresponds to the first quarter of 2000 and \\(t = T = 71\\) corresponds to the third quarter of 2017 (the last observation in the dataset). To compute the growth rates for each quarter compared to the same quarter during the previous year we will use a continuous growth rate defined as follows: \\[ growthRate_{t} = \\frac{y_{t} - y_{t - 1}}{y_{t - 4}} \\approx \\log(y_{t}) - \\log(y_{t - 4}) \\] ## First we take the 4-th lag of the series and afterwards we take the difference ## between the log values of the original series and the log values of the lag(4-th lag) growthSeries &lt;- log(gdpSeries) - log(lag(gdpSeries, k = 4)) 3.1 Data exploration In the previous section we have defined the growth rates we are about to model and we have computed the necessary growth rates series. Please note that the first four values of the growth series are missing (undefined), because the first quarter of 2001 is the first data point were we can apply the formula use to compute these growth rates. We cannot apply it to the last quarter of 2000, for example, because we lack data on the GDP in the last quarter of 1999. Before we begin to build our model let us examine the plot of the series as well as its autocorrelation and partial autocorrelation plots. ## mean(growthSeries, na.rm = TRUE) plot(growthSeries) ## As the growth series contains missing values we must instruct acf and pacf to ## exclude these, otherwise acf/pacf will throw errors. ## Draw the autocorrelation plot acf(growthSeries, na.action = na.omit) ## Draw the partial autocorrelation plot pacf(growthSeries, na.action = na.omit) The ACF plot show slowly decaying autocorrelations that disappear after approx. one year. This appears consistent with autoregressive models. The partial autocorrelation plot shows a single large and significat partial autocorrelation at lag 1. This is consistent with a first order autoregressive process so we will try fitting one as our next step. 3.2 Fit an AR(p) process Based on the exploratory analysis we have chosen a AR(1) model for the GDP growth data. \\[ y_{t} = \\delta + \\alpha y_{t - 1} + u_{t}, u_{t} \\sim WN(\\sigma^2): \\text{AR(1)} \\] This model has two coefficients that we need to estimate (learn) from the data: \\(\\delta\\) and \\(\\alpha\\). We will use the arima function to compute these estimates using a maximum likelihood method (not discussed in this class, but similar in spirit to the least squares method from the econometrics class). ## Similar to lm() ## ARIMA(p, d, q) fitAR1 &lt;- arima(growthSeries, order = c(1, 0, 0)) ## AR(1) fitAR1 ## ar1: alpha ## ## Call: ## arima(x = growthSeries, order = c(1, 0, 0)) ## ## Coefficients: ## ar1 intercept ## 0.8152 0.0353 ## s.e. 0.0665 0.0109 ## ## sigma^2 estimated as 0.0003066: log likelihood = 175.4, aic = -344.79 from the output of the arima function we obtain the estimates for \\(\\delta\\) and \\(\\alpha\\). \\[ \\hat{y}_{t + 1} = \\hat{\\delta} + \\hat{\\alpha} y_{t}\\\\ \\hat{\\alpha} = 0.8152\\\\ \\hat{\\delta} = 0.0353\\\\ \\hat{y_{t}} = 0.0353 + 0.8152\\hat{y}_{t - 1}\\\\ \\] To compute a one-period ahead prediction using the fitted model, use the predict function. ## Use predict to compute the one predict(fitAR1, n.ahead = 1) ## $pred ## Qtr4 ## 18 0.03716986 ## ## $se ## Qtr4 ## 18 0.01751059 We predict a 3.7% percent growth for the last quarter of 2017 (compared to the last quarter of 2016). The predict function outputs two values: the point prediction and the estimated standard error of this prediction. To express the uncertainty inherent in the prediction we compute an approx. 95 percent prediction interval using the standard error. An approximate 95% prediction interval for the growth rate in 2017Q4 is given by [0.03716986 - 20.01751059, 0.03716986 + 20.01751059] = [0.00214868, 0.07219104]. Note that ARIMA models in general are best suited for short-term predictions, because the quality of the prediction depends on the strength of the autocorrelations in the series (associations between present and past values). For periods far away from the latest observed value the ARIMA predictions collapse to the sample mean of the series. ## Use predict to compute the one predict(fitAR1, n.ahead = 50) ## $pred ## Qtr1 Qtr2 Qtr3 Qtr4 ## 18 0.03716986 ## 19 0.03681630 0.03652808 0.03629314 0.03610162 ## 20 0.03594551 0.03581825 0.03571451 0.03562994 ## 21 0.03556101 0.03550482 0.03545901 0.03542167 ## 22 0.03539124 0.03536642 0.03534620 0.03532971 ## 23 0.03531627 0.03530532 0.03529639 0.03528911 ## 24 0.03528317 0.03527834 0.03527439 0.03527118 ## 25 0.03526856 0.03526642 0.03526468 0.03526326 ## 26 0.03526210 0.03526116 0.03526039 0.03525977 ## 27 0.03525925 0.03525884 0.03525850 0.03525822 ## 28 0.03525800 0.03525781 0.03525766 0.03525754 ## 29 0.03525744 0.03525736 0.03525729 0.03525724 ## 30 0.03525720 0.03525716 0.03525713 0.03525711 ## 31 0.03525709 ## ## $se ## Qtr1 Qtr2 Qtr3 Qtr4 ## 18 0.01751059 ## 19 0.02259132 0.02541172 0.02712416 0.02820461 ## 20 0.02890024 0.02935336 0.02965063 0.02984652 ## 21 0.02997598 0.03006170 0.03011852 0.03015622 ## 22 0.03018125 0.03019787 0.03020890 0.03021624 ## 23 0.03022111 0.03022434 0.03022649 0.03022792 ## 24 0.03022887 0.03022950 0.03022992 0.03023020 ## 25 0.03023039 0.03023051 0.03023059 0.03023064 ## 26 0.03023068 0.03023070 0.03023072 0.03023073 ## 27 0.03023074 0.03023074 0.03023075 0.03023075 ## 28 0.03023075 0.03023075 0.03023075 0.03023075 ## 29 0.03023075 0.03023075 0.03023075 0.03023075 ## 30 0.03023075 0.03023075 0.03023075 0.03023075 ## 31 0.03023075 ## Sample mean mean(growthSeries, na.rm = TRUE) ## [1] 0.03524203 3.3 Model choice (1) Up to now we have discussed how to estimate the coefficients of a given ARIMA model using the arima function and how to compute predictions with predict, but we didn’t question whether the model can adequately describe our data or not. A model that fits the data poorly will likely result in inadequate predictions. Here we will discuss a graphical analysis of the model fit using tsdiag. In order to see how a poorly fitting model looks we will fit a AR(0) model to the data and examine it first. \\[ y_{t} = \\delta + u_{t} \\quad \\text{AR(0)} \\] fitAR0 &lt;- arima(growthSeries, c(0, 0, 0)) fitAR0 ## ## Call: ## arima(x = growthSeries, order = c(0, 0, 0)) ## ## Coefficients: ## intercept ## 0.0352 ## s.e. 0.0038 ## ## sigma^2 estimated as 0.0009676: log likelihood = 137.44, aic = -270.89 After fitting the AR(0) model we will examine its residuals, defined as: \\[ r_{t} = y_{t} - \\hat{y}_t \\] tsdiag(fitAR0) The tsdiag function produces three plots. The first one (top) show the standardised residuals of the model fit. For a good fit we expect to see standardised residuals that fluctuate around zero with a constant variance (fluctuations). This is not the case for the AR(0) fit. We that the residuals are positive (i.e. our model systematically underestimates the growth rates) before 2009 and are negative afterwards (i.e. our model systematically overestimates the growth rates). This points to a poor model choice. From the plot of the growth series data it is easy to see why this is happening. The AR(0) model has a single coefficient (\\(\\delta\\)) and it effectively predicts each value of the series with the sample mean of the series. ## Plot the growth series ## Here we use plot.zoo instead of plot.xts as we usually do, because it ## is easier to draw the horizontal line at the sample mean of the series. plot.zoo(growthSeries) ## Draw a horizontal line at the mean (0.03524) abline(h = mean(growthSeries, na.rm = TRUE), col = 2) Because of the large shift after the financial crisis of 2009 the sample mean underestimates the growth rates before 2009 and overestimates the growth rates after that. The next two plots are related and they show the autocorrelations of the residuals. Good model should not have residuals with significant autocorrelations. For the AR(0) models the residuals are strongly correlated (as seen from the bars in the second plot) and the autocorrelation disappears only after the first year (4-th lag). The last plot show p-values of the Ljung-Box autocorrelations tests. The first point refers to the p-value of the Ljung-Box test for the first-order autocorrelation. It test the hypothesis: \\[ H_{0}: \\rho_{1} = 0\\\\ H_{1}: \\rho_{1} \\neq 0 \\] The p-value of this test lies below the blue line in the plot which is drawn at 0.05. As the p-value is less than 0.05 we would reject the null hypothesis. A good model should not allow us to reject this hypothesis. The second point in the last plot corresponds to Ljung-Box test of the hypothesis: \\[ H_{0}: \\rho_{1} = \\rho_{2} = 0\\\\ H_{1}: \\text{ at least one of } \\rho_{1}, \\rho_{2} \\neq 0\\\\ \\] Again, the p-value of this test leads us to reject the null hypothesis. Let us now turn to the diagnostic of our initial choice of a model, the AR(1) model. tsdiag(fitAR1) We see that the first plot looks much better than the one of the AR(0) model as we do not observe systematic overestimation or underestimation. Then next two plots also indicate an adequate fit as there are no significant autocorrelations in the residuals. 3.4 Model choice (2) Up until now we have examined the goodness of fit diagnostic of the AR(1) process and concluded that there are no significant autocorrelations in its residuals. However, we could raise the question whether we could fare better (i.e. describe the data better) with a more complex model, for example AR(2). The AR(2) model has one more parameter than the AR(1) model and is better able to fit more closely to the data. \\[ y_{t} = \\delta + \\alpha_1 y_{t - 1} + \\alpha_2 y_{t - 2} + u_{t} \\] "]]
