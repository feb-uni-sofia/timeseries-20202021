<!-- \newcommand{\bar}[1]{\bar{#1}} -->
```{r}
library(xts)
```

# Sample covariance and sample correlation {#covariance}

## Time series analysis class 2

Let us denote the realisations of a time series process with
$$
y: y_1, y_2,\ldots,y_T
$$

where $y_1$ is the first value of the series and $y_T$ is the last value of the series.

The first lag of the series is defined as 

$$
y_{t - 1}: \text{ first lag}
$$


## Purely random process (white noise)

Let $u_t, t = 1\ldots,T$ be an uncorrelated, normally distributed zero mean random variables with constant variance $\sigma^2$.

$$
u_{t} \sim N(0, \sigma^2), \quad t=1,\ldots,T
$$
We call $u_{t}$ a purely random or white noise process.

$$
E(u_{t}) = 0\\
Var(u_{t}) = \sigma^2\\
Cov(u_{t}, u_{t - k}) = 0 \quad \text{for} \quad k \neq 0
$$

## Simulating white noise

Generate 100 values from a standard normal distribution (i.e. $\sigma^2  = 1$) and create an arbitrary time index (e.i. 100 days starting from 2018-10-10).

```{r}
library(xts)
## Select 100 values at random from the standard normal distribution (the defaults in rnorm are mean = 0 and sd = 1).
randomValues <- rnorm(100)

## Create a time index for xts
timeIndex <- seq.Date(as.Date("2018-10-10"), by = "day", length.out = 100)
## Combine the values and the time index
u <- xts(randomValues, timeIndex)
## Plot the series
plot(u)
```
From the plot you can wee that the series fluctuates around its mean (close to 0) and appears to have a constant variance. Most of the values lie between -2 and 2 as expected for values selected from a standard normal distribution.

$$
X \sim N(\mu, \sigma^2)\\
\text{ roughly } 95\% \text{ of the realisations of X are expected to be in the interval} \\
[\mu - 2\sigma, \mu + 2\sigma] \\
\text{for } \mu = 1, \sigma^2 = 1\quad (\text{standard normal distribution})\\
\text{we obtain } [0 - 2*1, 0 + 2 * 1] = [-2, 2]
$$
To examine the correlation between the series and its first lag we create a scatter-plot of $u_{t}$ and $u_{t - 1}$.


```{r}
## Create the first lag (u_{t - 1})
uL1 <- lag(u)
## Next we combine the original series and the lag so that it is easier for plotting.
combinedSeries <- cbind(u, uL1)

## Finally we create the scatter-plot
plot(as.data.frame(uL1)[, 1], as.data.frame(u)[, 1], xlab = "u_{t - 1}", ylab = "u_{t}", main = "Scatterplot of u_t and u_{t - 1}")
```

The plot indicates no association between $u_{t}$ and $u_{t - 1}$.


## Sample (empirical) covariance

A key statistic for measuring the _linear_ association between time series is the correlation coefficient. To define it we first examine the sample _covariance_ between two series $x_{t}$ and $y_{t}$.

Let $x$ and $y$ be two random variables with $T$ realisations.
$$
x: x_{1}, x_{2}, \ldots, x_{T}\\
y: y_{1}, y_{2}, \ldots, y_{T}\\
$$
The sample covariance is defined as:
$$
Cov(x, y) = \frac{1}{T - 1}\sum_{t = 1}^{T}(x_t - \overline{x})(y_{t} - \overline{y})
$$

In order to see how it works we will examine the results from a couple of simulations. 

### Positive linear dependency

First, let us define the relationship between $x_{t}$ and $y_{t}$ as follows.

$$
y_{t} = 1 + x_{t} + u_{t}, \quad u_{t} \sim WN(\sigma^2 = 5)
$$
The above equation simply tells us that the value of $y_{t}$ is simply the value of $x_{t}$ plus one and the result of a normally distributed random variable. For simplicity, let $x_{t} = t$.


```{r}
## Generate a vector of values for x: the integers from 1 to 50.
x <- 1:50
## Draw values at random from a normal distribution with mean zero and variance 25
noise <- 5*rnorm(50)
## Finally, generate the value for y according to the equation above
y <- 1 + x + noise

## Plot x and y
plot(x, y)
## Compute the mean of x and the mean of y
mean(x)
mean(y)

## Draw a (red) vertical line at the mean of x
abline(v = mean(x), col = 3, lwd = 2)

## Draw a (green) horizontal line at the mean of y
abline(h = mean(y), col = 2, lwd = 2)
```
From the plot you will notice that most of the points fall in the lower-left and in the upper-right parts of the plot. Next examine the sign of the cross products $(x_t - \overline{x})(y_t - \overline{y})$ that we find in the definition of the covariance.

$$
\overline{x} = \frac{1}{T}\sum_{t = 1}^{T}x_{t} \text{ the sample mean of } x \\
\overline{y} = \frac{1}{T}\sum_{t = 1}^{T}y_{t} \text{ the sample mean of } y \\
(x_t - \overline{x})(y_{t} - \overline{y}) > 0 \text{ for points in the top right part}\\
(x_t - \overline{x})(y_{t} - \overline{y}) > 0 \text{ for points in the bottom left part}\\
(x_t - \overline{x})(y_{t} - \overline{y}) < 0 \text{ for points in the bottom right and top left parts}\\
(y_t - \overline{y}) > 0 \text{ for points above the red line}\\
(y_t - \overline{y}) < 0 \text{ for points below the red line}\\
(x_t - \overline{x}) > 0 \text{ for points to the right of the green line}\\
(x_t - \overline{x}) < 0 \text{ for points to the left of the green line}
$$
You should notice that for a positive linear associaiton between $x$ and $y$ the cross products will be positive and therefor the covariance of $x$ and $y$ will be positive.

You can compute the sample covariance between the $x$ and $y$ by using the `cov` function:

```{r}
## Sample covariance of x and y
cov(x, y)
```

To see how the sign of the covariance changes for negative linear associations between $x$ and $y$, change the sign before $x$ and re-run the simulation.

Unfortunately, the value of the covariance depends on the units of measurement of $x$ and $y$ (i.e. kg, mg, etc.). Try multiplying x or y or both by a constant and see how the covariance changes.

```{r}
cov(x, 50*y)
cov(10*x, y)
cov(10*x, 10*y)
```

The dependency of the covariance on the unit of measurement of the variables makes it difficult to distinguish between strong and weak dependence. Therefore we introduce the correlation coefficient $\rho(x, y)$. It is defined as the ratio of the covariance and the standard deviations of the variables.

$$
s^2(x) = \frac{1}{T - 1}\sum_{t = 1}^{T}(x_t - \overline{x})^2 \quad \text{sample variance of x}\\
s^2(y) = \frac{1}{T - 1}\sum_{t = 1}^{T}(y_t - \overline{y})^2 \quad \text{sample variance of x} \\
s(x) = \sqrt{s^2(x)} \quad \text{sample standard deviation of x}\\
s(y) = \sqrt{s^2(y)} \quad \text{sample standard deviation of y}\\

\rho(x, y) = \frac{Cov(x, y)}{s(x)s(y)} \text{ correlation coefficient}
$$
It can be shown that the correlation coefficient is always less than or equal to 1 and greater than or equal to -1.

$$
-1 \leq \rho(x, y) \leq 1
$$

It equals 1 for a perfect linear association between $x$ and $y$. 

$$
y_{t} =  x_{t}
$$

$$
\rho(x, y) = \rho(x, x) = \frac{Cov(x, x)}{\sqrt{s^2(x)s^2(x)}} = \frac{s^2(x)}{s^2(x)} = 1
$$
In the above we use the fact that the covariance between $x$ and $x$ is simply the variance of $x$:


$$
Cov(x, x) = \frac{1}{T - 1}\sum_{t = 1}^{T}(x_t - \overline{x})(x_t - \overline{x}) =\\ \frac{1}{T - 1}\sum_{t = 1}^{T}(x_t - \overline{x})^2 = Var(x) = s^2(x)
$$


The correlation coefficient equals -1 for a perfect negative linear association:

$$
y_{t} = -x_{t} \implies \frac{Cov(-x, x)}{\sqrt{s^2(-x)s^2(x)}} = -\frac{Cov(x, x)}{\sqrt{s^2(x)s^2(x)}} = -\frac{s^2(x)}{s^2(x)} = -1
$$

The above result also holds for other linear associations:

$$
y = a + bx, b > 0\implies \rho(x, y) = 1\\
y = a + bx, b < 0\implies \rho(x, y) = -1\\
$$
## Autocovariances and autocorrelations

In the case of time series we are interested in describing the dependency pattern 
between the value of a series and its past values (lags).
$$
y_t, y_{t - 1}, y_{t - 2},\ldots, y_{t - k}
$$

$$
\gamma_0 = Cov(y_t, y_{t}) = Var(y_t) = s^2(x)\\
\gamma_{1} = Cov(y_{t}, y_{t - 1}) \text{ first order autocovariance }\\
\gamma_{2} = Cov(y_{t}, y_{t - 2}) \text{ second order autocovariance } \\
\gamma_{3} =Cov(y_{t}, y_{t - 3}) \text{ third order autocovariance } \\
\vdots \\
\gamma_{k} =Cov(y_{t}, y_{t - k}) \text{ k-th order autocovariance } \\
$$

$$
\rho_{0}= \rho(y_t, y_{t}) = 1 \\
\rho_{1} = \rho(y_{t}, y_{t - 1}) \text{ first order autocorrelation }\\
\rho_{2} = \rho(y_{t}, y_{t - 2}) \text{ second order autocorrelation } \\
\rho_{3} =\rho(y_{t}, y_{t - 3}) \text{ third order autocorrelation } \\
\vdots \\
\rho_{k} =\rho(y_{t}, y_{t - k}) \text{ k-th order autocorrelation }
$$

$$
\rho_1 = \frac{\gamma_1}{\gamma_0} \\
\rho_2 = \frac{\gamma_2}{\gamma_0} \\
\vdots\\
\rho_k = \frac{\gamma_k}{\gamma_0}
$$

## Exercise

Compute the sample first order autocovariance of the following time series.

$$
x: x_1, x_2, x_3\\
x: (2, 3, 10)\\

Cov(x_t, x_{t - 1}) = \frac{1}{T - 1}\sum_{t = 1}^{T}(x_t - \overline{x})(x_{t - 1} - \overline{x})\\
\overline{x} = 5\\
\gamma_1 = Cov(x_t, x_{t - 1}) = \frac{1}{3 - 1}[(2 - 5)(NA - 5) + (3 - 5)(2 - 5) + (10 - 5)(3 - 5)] =\\
= \frac{1}{2}[NA + (-2)(-3) + (5)(-2)] = \frac{1}{2}(6 -10) = -2\\
\gamma_1 = -2\\
\rho_{1} = ? \\


s^2(x) = Var(x_{t}) = \frac{1}{T - 1}\sum_{t = 1}^{T}\left[(x_t - \overline{x})^2\right] =\\
\frac{1}{3 - 1}[(2 - 5)^2 + (3 - 5)^2 + (10 - 5)^2] = \frac{1}{2}[(-3)^2 + (-2)^2 + 5^2] = \\
\frac{1}{2}[9 + 4 + 25] = \frac{38}{2} = 19\\
\gamma_0 = 19\\
\rho_{1} = \frac{\gamma_1}{\gamma_0} = \frac{-2}{19} = ...
$$

We can check that our result is correct by running the same calculation with R:

```{r}
var(c(2, 3, 10))
acf(c(2, 3, 10), plot = FALSE)
```

# Fitting arima models {#fit}

Let us look at a real dataset of Bulgarian quarterly GDP figures from 2000 to 2017. Our goal will
be to select a mathematical model for the quarterly GDP growth rates.

```{r}
library(xts)
# a) Read data
gdp <- read.csv('https://raw.githubusercontent.com/feb-uni-sofia/timeseries-20202021/main/data/gdp_bg_qrt_2000-2017.csv')

## Create a time index from the column names "Index" in the gdp dataset.
timeIndex <- as.yearqtr(gdp$Index, format = '%YQ%q')
## Combine the values of the GDP (column called "GDP") and the time index to create
## a time series object.
gdpSeries <- xts(gdp$GDP, order.by = timeIndex)
```

Before we compute the quarterly growth rates we will first examine the original series.

```{r}
plot(gdpSeries)
acf(gdpSeries)
```
A prominent feature in the dataset is a strong seasonal effect. Furthermore, the data appears to have a positive trend at least until 2009.

Let us denote the data in this time series with
$$
y_{1}, y_{2}, y_{3},\ldots,y_{T}\\
$$
where $t = 1$ corresponds to the first quarter of 2000 and $t = T = 71$ corresponds to the third quarter of 2017 (the last observation in the dataset).

To compute the growth rates for each quarter compared to the same quarter during the previous year we will use a continuous growth rate defined as follows:

$$
growthRate_{t} = \frac{y_{t} - y_{t - 1}}{y_{t - 4}} \approx \log(y_{t}) - \log(y_{t - 4})
$$
```{r}
## First we take the 4-th lag of the series and afterwards we take the difference
## between the log values of the original series and the log values of the lag(4-th lag)
growthSeries <- log(gdpSeries) - log(lag(gdpSeries, k = 4))
```


## Data exploration

In the previous section we have defined the growth rates we are about to model and we 
have computed the necessary growth rates series. Please note that the first four values
of the growth series are missing (undefined), because the first quarter of 2001 is the first 
data point were we can apply the formula use to compute these growth rates. We cannot apply it to the last quarter of 2000, for example, because we lack data on the GDP in the last quarter of 1999.


Before we begin to build our model let us examine the plot of the series as well as
its autocorrelation and partial autocorrelation plots.

```{r}
## mean(growthSeries, na.rm = TRUE)
plot(growthSeries)

## As the growth series contains missing values we must instruct acf and pacf to 
## exclude these, otherwise acf/pacf will throw errors.

## Draw the autocorrelation plot
acf(growthSeries, na.action = na.omit)

## Draw the partial autocorrelation plot
pacf(growthSeries, na.action = na.omit)
```

The ACF plot show slowly decaying autocorrelations that disappear after approx. one year. This appears consistent with autoregressive models. The partial autocorrelation plot shows a single large and significat partial autocorrelation at lag 1. This is consistent with a first order autoregressive process so we will try fitting one as our next step.

## Fit an AR(p) process

Based on the exploratory analysis we have chosen a AR(1) model for the GDP growth data.

$$
y_{t} = \delta + \alpha y_{t - 1} + u_{t}, u_{t} \sim WN(\sigma^2): \text{AR(1)}
$$


This model has two coefficients that we need to estimate (learn) from the data: $\delta$ and $\alpha$. We will use the `arima` function to compute these estimates using a maximum likelihood method (not discussed in this class, but similar in spirit to the least squares method from the econometrics class).

```{r}
## Similar to lm()
## ARIMA(p, d, q)
fitAR1 <- arima(growthSeries, order = c(1, 0, 0)) ## AR(1)
fitAR1 ## ar1: alpha
```

from the output of the `arima` function we obtain the estimates for $\delta$ and $\alpha$. 

$$
\hat{y}_{t + 1} = \hat{\delta} + \hat{\alpha} y_{t}\\
\hat{\alpha} = 0.8152\\
\hat{\delta} = 0.0353\\
\hat{y_{t}} = 0.0353  + 0.8152\hat{y}_{t - 1}\\
$$
To compute a one-period ahead prediction using the fitted model, use the `predict` function.

```{r}
## Use predict to compute the one 
predict(fitAR1, n.ahead = 1)
```

We predict a 3.7% percent growth for the last quarter of 2017 (compared to the last quarter of 2016).
The `predict` function outputs two values: the point prediction and the estimated standard error of this prediction. To express the uncertainty inherent in the prediction we compute an approx. 95 percent prediction interval using the standard error.

An approximate 95% prediction interval for the growth rate in 2017Q4 is given
by [0.03716986 - 2*0.01751059, 0.03716986 + 2*0.01751059] = [0.00214868, 0.07219104].


Note that ARIMA models in general are best suited for short-term predictions, because the quality of the prediction depends on the strength of the autocorrelations in the series (associations between present and past values). For periods far away from the latest observed value the ARIMA predictions collapse to the sample mean of the series.

```{r}
## Use predict to compute the one 
predict(fitAR1, n.ahead = 50)

## Sample mean
mean(growthSeries, na.rm = TRUE)
```
## Model choice (1)

Up to now we have discussed how to estimate the coefficients of a given ARIMA model using the `arima` function and how to compute predictions with `predict`, but we didn't question whether the model can adequately describe our data or not. A model that fits the data poorly will likely result in inadequate predictions.

Here we will discuss a graphical analysis of the model fit using `tsdiag`. In order to see how a poorly fitting model looks we will fit a AR(0) model to the data and examine it first.

$$
y_{t} = \delta + u_{t} \quad \text{AR(0)}
$$

```{r}
fitAR0 <- arima(growthSeries, c(0, 0, 0))
fitAR0
```

After fitting the AR(0) model we will examine its residuals, defined as: 

$$
r_{t} = y_{t} - \hat{y}_t
$$
```{r}
tsdiag(fitAR0)
```

The `tsdiag` function produces three plots. The first one (top) show the standardised residuals of the model fit. For a good fit we expect to see standardised residuals that fluctuate around zero with a constant variance (fluctuations). This is not the case for the AR(0) fit. We that the residuals are positive (i.e. our model systematically underestimates the growth rates) before 2009 and are negative afterwards (i.e. our model systematically overestimates the growth rates). This points to a poor model choice. From the plot of the growth series data it is easy to see why this is happening. The AR(0) model has a single coefficient ($\delta$) and it effectively predicts each value of the series with the sample mean of the series.

```{r}
## Plot the growth series
## Here we use plot.zoo instead of plot.xts as we usually do, because it
## is easier to draw the horizontal line at the sample mean of the series.
plot.zoo(growthSeries)

## Draw a horizontal line at the mean (0.03524)
abline(h = mean(growthSeries, na.rm = TRUE), col = 2)
```
Because of the large shift after the financial crisis of 2009 the sample mean underestimates 
the growth rates before 2009 and overestimates the growth rates after that.


The next two plots are related and they show the autocorrelations of the residuals. Good model should not have residuals with significant autocorrelations. For the AR(0) models the residuals are strongly correlated (as seen from the bars in the second plot) and the autocorrelation disappears only after the first year (4-th lag).

The last plot show p-values of the Ljung-Box autocorrelations tests. The first point refers to the p-value of the Ljung-Box test for the first-order autocorrelation. It test the hypothesis:

$$
H_{0}: \rho_{1} = 0\\
H_{1}: \rho_{1} \neq 0
$$
The p-value of this test lies below the blue line in the plot which is drawn at 0.05. As the p-value is less than 0.05 we would reject the null hypothesis. A good model should not allow us to reject this hypothesis.


The second point in the last plot corresponds to Ljung-Box test of the hypothesis:

$$
H_{0}: \rho_{1} = \rho_{2} = 0\\
H_{1}: \text{ at least one of } \rho_{1}, \rho_{2} \neq 0\\
$$

Again, the p-value of this test leads us to reject the null hypothesis.



Let us now turn to the diagnostic of our initial choice of a model, the AR(1) model.

```{r}
tsdiag(fitAR1)
```

We see that the first plot looks much better than the one of the AR(0) model as we do not observe
systematic overestimation or underestimation.

Then next two plots also indicate an adequate fit as there are no significant autocorrelations in the residuals.


## Model choice (2)

Up until now we have examined the goodness of fit diagnostic of the AR(1) process and concluded that there are no significant autocorrelations in its residuals. However, we could raise the question whether we could fare better (i.e. describe the data better) with a more complex model, for example AR(2).

The AR(2) model has one more parameter than the AR(1) model and is better able to fit more closely to the data.

When selecting a model for a time series our goal is to find the most parsimonious model (less coefficients) that can still adequately describe the structure of the data (mean, variance, autocorrelations). One model summary statistic that can aid us in this choice is the Akaike Information Criterion (AIC). Its value is low when the model fits the data closely but is penalised for the number of coefficients estimated in that model. Thus when using the AIC we will select the model with the lowest AIC value.


# Dickey-Fuller unit root tests

## No level, no trend

Consider a simple first order autoregressive model (AR(1)) with the constant $\delta = 0$.

$$
y_{t} = \alpha_1 y_{t - 1} + u_{t}
$$
We already know that for $|alpha_1| < 1$ the process is stationary (i.e. its statistical properties: mean, variance and auto-covariances do not depend on the time index $t$.). For $\alpha_1 = 1$ the process is non-stationary and behaves quite differently (see the simulation below)


```{r}
library(xts)
n <- 1000
## Select 1000 observations at random from the standard normal distribution
## (the default for rnorm)
u <- rnorm(n)

## Sum the selected values cumulatively, i.e. randomWalk[1] = u[1], randomWalk[2] = u[1] + u[2], etc.
randomWalk <- cumsum(u)

## Simulate a stationary AR(1) process with alpha1 = 0.7
alpha1 <- 0.7
stationaryAR1 <- arima.sim(n = n, model = list(ar = c(alpha1)))
timeIndex <- seq(as.Date("2020-10-10"), by ="day", length.out = n)
## Combine the random walk and the AR(1) series into a data frame
## and pass that data frame to xts. We do this, because it is easier to 
## plot the two series in _one_ plot when calling plot
series <- xts(data.frame(randomWalk, stationaryAR1), order.by = timeIndex)
plot(series, main = paste("Random walk and AR(1) with alpha1 = ", alpha1))
```

From the plot you should notice that the stationary series fluctuates around its expected value (0) whereas the random walk series behaves quite differently without an obvious tendency to revert to its mean.


For simplicity let us assume that we have observed a time series from a AR(1) model with an unknown coefficients $\alpha_1$.

We would like to test the hypothesis that the coefficient $\alpha_1 = 1$ (i.e. the process is a random walk) against the one-sided alternative that it is less than 1. Looking at the model equation we could think about using a simple t-test as we did in the introductory econometrics course. It is convenient to first transform the model by subtracting $y_{t - 1}$ from both sides of the equation, because the default t-tests produced from R functions
used to fit the model to the data test the null hypothesis that the coefficient is zero.

$$
y_{t} - y_{t - 1} = \alpha_1 y_{t - 1} - y_{t - 1} + u_{t} \implies \\
y_{t} - y_{t - 1} = (\alpha_1 - 1)y_{t - 1} + u_{t}\\
\Delta y_{t} = (\alpha_1 - 1)y_{t - 1} + u_{t}
$$


With the transformed model we can now restate the unit root null hypothesis.

$$
H_{0}: \alpha_1 = 1 \Leftrightarrow H_{0}: \alpha_1 - 1 = 0
$$
Instead of fitting the equation with the `lm` that we used in the econometrics course we will use a special function (`ur.df`) that is available from the `urca` package. We do this because the usual statistical theory behind the t-test for the regression coefficients does not hold under the null hypothesis and we need a special set of critical values (Dickey-Fuller critical values).

It is easiest to see how the test works with an example. First we generate 1000 observations from a stationary AR(1) process with $\alpha = 0.7$. Then we plot the series and perform the t-test using the Dickey-Fuller critical values produced by `ur.df`.

```{r}
library(urca)
set.seed(123)
## Set the simulation size (number of observations)
n <- 1000

##
alpha1 <- 0.7
stationaryAR1 <- arima.sim(n = n, model = list(ar = c(alpha1)))
plot(stationaryAR1, main = paste("Stationary AR(1) process with apha1 = ", alpha1))
summary(ur.df(stationaryAR1, selectlags = "Fixed", lags = 0, type = "none"))
```
You can find the critical values of the test in the `tau1` row in the critical values section of the `ur.df` output.
To perform the test, compare the value of the t-statistic (t-value in the `z.lag.1` row) with the set of critical values. We reject the null hypothesis that the process has a unit root for _small_ values of the t-statistic. In this example the t-statistic equals -13.94 and is less than the critical value at 1pct (-2.58), so we reject the null hypothesis. For this series this is the expected result, because we know that there is no unit-root in our generated data ($\alpha_1 = 0.7$).


Next, let us look at data generated from a random-walk process and let us apply the Dickey-Fuller test.

```{r}

library(xts)
n <- 1000
## Select 1000 observations at random from the standard normal distribution
## (the default for rnorm)
u <- rnorm(n)

## Sum the selected values cumulatively, i.e. randomWalk[1] = u[1], randomWalk[2] = u[1] + u[2], etc.
randomWalk <- cumsum(u)

timeIndex <- seq(as.Date("2020-10-10"), by ="day", length.out = n)
rwSeries <- xts(randomWalk, order.by = timeIndex)
plot(rwSeries, main = "Data generated from a random walk process")
summary(ur.df(rwSeries, selectlags = "Fixed", lags = 0, type = "none"))
```

In the case of the random-walk data we cannot reject the null hypothesis (has unit-root), because the value of the t-statistic is -0.691 and it is greater even than the critical value at 10 percent (-1.62).


## Non-zero level, no trend

An important aspect of using the Dickey-Fuller test is that it is quite sensitive to deviations from its assumptions about the deterministic parts of the model (level and trend). Up to now our regression equation was:

$$
\Delta y_{t} = (\alpha_1 - 1)y_{t - 1} + u_{t}
$$
and it is appropriate for data without a trend that fluctuate around zero.

Let us see what happens when we perform the Dickey-Fuller for data with non-zero level. Again, it is easier to use a simulation to illustrate the problem. We will use the same data from the AR(1) model with $\alpha_1 = 0.7$ from the previous example and add a constant to all the values.


```{r}
stationaryAR1WithLevel <- stationaryAR1 + 100
plot(stationaryAR1WithLevel, main="AR(1) with alpha_1 = 0.7 and delta = 100")
```

We see that the only difference from the previous example is that the values of the AR(1) data fluctuate around 100 instead of 0. 

```{r}
summary(ur.df(stationaryAR1WithLevel, selectlags = "Fixed", lags = 0, type = "none"))
```

Although there is no unit root in the data we see that the Dicke-Fuller test fails to reject the null hypothesis, because -0.1937 (t-statistic) > -1.62 (critical value). This happens because we need to account for the non-zero level in the regression equation. Adding a constant it now becomes:

$$
\Delta y_{t} = \delta + (\alpha_1 - 1)y_{t - 1} + u_{t}
$$
To use this equation when testing, set the `type = "drift"` option in `ur.df`.

```{r}
summary(ur.df(stationaryAR1WithLevel, selectlags = "Fixed", lags = 0, type = "drift"))
```
Now compare the value of the t-statistic (-13.95) to the first set of critical values (tau2 -3.43 -2.86 -2.57). This time the test rejects the null hypothesis of a unit root at any reasonable significance level, because the value of the t-statistic is less than the critical value at 1pct (-3.43).


## Linear trend

Apart from a non-zero level we often want to perform unit-root tests with data that has trend (systematic increase/decrease with time). Let us look again at the AR(1) data with $\alpha_1 = 0.7$ and add a linear trend to it. Afterwards we will perform the Dickey-Fuller test and see how it performs.


```{r}
linearTrend <- 0.06*(1:n)
trendStationary <- linearTrend + stationaryAR1
plot(trendStationary, main="AR(1) with alpha_1 = 0.7 and a linear trend.")
summary(ur.df(trendStationary, selectlags = "Fixed", lags = 0, type = "drift"))
```

From the output of `ur.df` we see that the Dickey-Fuller test with drift that we 
used in the previous example fails to reject the null hypothesis of a unit root, because
the value of the t-statistic (-1.0165) is greater that the critical value at 10pct (-2.57). This is unexpected, because we know that there is no unit root in our generated data. This result happens here, because we havn't accounted for the linear trend in our regression equation.

Let us add a term for the linear trend.

$$
\Delta y_{t} = \delta + \beta t + (\alpha_1 - 1)y_{t - 1} + u_{t}
$$
To use the trend equation with `ur.df`, specify the `type="trend"` option.

```{r}
summary(ur.df(trendStationary, selectlags = "Fixed", lags = 0, type = "trend"))
```

We see that adjusting the regression equation leads the Dickey-Fuller test to reject the null hypothesis of unit root, because the value of the t-statistic (-13.948) is less even than the critical value at 1pct (-3.96).




